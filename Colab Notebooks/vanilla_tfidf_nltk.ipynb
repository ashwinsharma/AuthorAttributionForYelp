{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "vanilla_tfidf_nltk.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcMIjgsSyRRh",
        "colab_type": "text"
      },
      "source": [
        "Mount Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QG81JmFUNbk",
        "colab_type": "code",
        "outputId": "a9fbd8c6-7fae-4acf-cfca-d9cbdfddc461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "data_dir = 'gdrive/My Drive/Colab Notebooks/AuthorshipAttribution/data' # @param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsOh1s3wyUCf",
        "colab_type": "text"
      },
      "source": [
        "Import stuff."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfjN_1sLUUTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import codecs\n",
        "import operator\n",
        "import re\n",
        "import string\n",
        "import argparse\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilhGcXq3UNbp",
        "colab_type": "code",
        "outputId": "14dc12d1-60e3-4f4f-9999-5aabac5df8d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, WordPunctTokenizer,PunktSentenceTokenizer, TreebankWordTokenizer\n",
        "from nltk.corpus import stopwords, webtext\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# following 2 files need to be downloaded if not already present for this to work.\n",
        "# nltk.download('webtext')\n",
        "# nltk.download('vader_lexicon')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgORqoixUNbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale\n",
        "from sklearn import utils\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC ,SVC\n",
        "from sklearn import preprocessing\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx12WupoyWy_",
        "colab_type": "text"
      },
      "source": [
        "Downloading essential NLTK material."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oboKX4mUNbu",
        "colab_type": "code",
        "outputId": "610f9d63-a744-40cc-a74d-d2958425d980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('webtext')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "# tokenize based on punctuation using nltk\n",
        "punk_sent_tokenizer = PunktSentenceTokenizer(webtext.raw('overheard.txt'))\n",
        "\n",
        "# nltk's built-in sentiment analyzer\n",
        "vader = SentimentIntensityAnalyzer()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0U2Wwx1yfXH",
        "colab_type": "text"
      },
      "source": [
        "Proprocessing: remove punctuation, remove stopwords, stem the tokens, convert all resulting tokens to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYzmejaTUNbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text pre-processing for the tf-idf model\n",
        "def tfidf_Preprocessing(text , _stopwords):\n",
        "    # stemming - using nltk's PorterStemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    text = text.replace('-' , '')\n",
        "    text = text.replace('.' , '')\n",
        "    text = text.replace('”' , '')\n",
        "    text = text.replace('’' , '')\n",
        "    text = text.replace('“' , '')\n",
        "    text = text.replace('‘' , '')\n",
        "    text = text.replace('–','')\n",
        "    nopunc = [char for char in text if char not in string.punctuation]\n",
        "    nopunc = ''.join(nopunc)\n",
        "    return ' '.join([stemmer.stem(word).lower() for word in WordPunctTokenizer().tokenize(nopunc) \n",
        "                    if stemmer.stem(word).lower() not in _stopwords])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbuk7c9By6rW",
        "colab_type": "text"
      },
      "source": [
        "The following two functions work together to extract the vocabulary from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqxCJrevUNbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculating word frequencies for a review text\n",
        "def tfidf_represent_text(text ):\n",
        "    tokens = WordPunctTokenizer().tokenize(text)\n",
        "    frequency = defaultdict(int)\n",
        "    for token in tokens:\n",
        "        frequency[token] += 1\n",
        "    return frequency"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfsAdFYcUNb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# constructing word vocabulary from texts - calculating word frequencies and storing them in dictionaries\n",
        "def tfidf_extract_vocabulary(texts , ft):\n",
        "    occurrences=defaultdict(int)\n",
        "    for text in texts:\n",
        "        text_occurrences=tfidf_represent_text(text)\n",
        "        for ngram in text_occurrences:\n",
        "            if ngram in occurrences:\n",
        "                occurrences[ngram]+=text_occurrences[ngram]\n",
        "            else:\n",
        "                occurrences[ngram]=text_occurrences[ngram]\n",
        "    vocabulary=[]\n",
        "    for i in occurrences.keys():\n",
        "        if occurrences[i]>=ft:\n",
        "            vocabulary.append(i)\n",
        "    return vocabulary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZFqvhoAzVFw",
        "colab_type": "text"
      },
      "source": [
        "We use the MaxAbsScaler to scale each feature by its Maximum Absolute Value. It translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r7uEbqHUNb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
        "stopwords_list = {'en': set(stopwords.words('english')) , 'fr':set(stopwords.words('french')),\n",
        "                  'sp': set(stopwords.words('spanish')) , 'it':set(stopwords.words('italian'))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFtnA1k807av",
        "colab_type": "text"
      },
      "source": [
        "Load the training and testing data and preprocess them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94kgs_HrUNb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# placeholders for training and testing data\n",
        "train_set , train_labels = [], []\n",
        "test_set , test_labels = [] , []\n",
        "\n",
        "# reading training and testing data from pickle files\n",
        "train_data, test_data = None, None\n",
        "with open(data_dir + \"/train_data.pickle\", \"rb\") as f:\n",
        "    train_data = pickle.load(f)\n",
        "with open(data_dir + \"/test_data.pickle\", \"rb\") as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "# populating training and testing placeholders with data\n",
        "train_set = train_data['train_texts']\n",
        "train_labels = train_data['train_labels']\n",
        "test_set = test_data['test_texts']\n",
        "test_labels = test_data['test_labels']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONZWSx1jUNb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_chunks(l, n):\n",
        "    n = max(1, n)\n",
        "    return [l[i:i+n] for i in range(0, len(l), n)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6eNZzhkUNb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 182 words is quite short\n",
        "# Try to join 5 tests texts together\n",
        "longer_test_texts = get_chunks(test_set, 5)\n",
        "longer_test_labels = get_chunks(test_labels, 5)\n",
        "longer_test_texts = ['\\n'.join(chunk) for chunk in longer_test_texts]\n",
        "longer_test_labels = [chunk[0] for chunk in longer_test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXfX0_H8UNcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converting the text into tf-idf vectors\n",
        "tfidf_train_set = [tfidf_Preprocessing(text , stopwords_list['en']) \n",
        "                for text in train_set]\n",
        "tfidf_test_set = [tfidf_Preprocessing(text , stopwords_list['en'])\n",
        "                for text in longer_test_texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r67i3-jZUNcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training and predicting using tf-idf model (which uses linear SVC)\n",
        "ft = 5\n",
        "tfidf_vocab = tfidf_extract_vocabulary(tfidf_train_set , ft )\n",
        "tfidf_vectorizer = TfidfVectorizer(vocabulary=tfidf_vocab, norm=None, strip_accents=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8W4ZqMiUNcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_train_data = tfidf_vectorizer.fit_transform(tfidf_train_set)\n",
        "tfidf_test_data = tfidf_vectorizer.fit_transform(tfidf_test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vwjo2PFUNcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_scaled_train_data = max_abs_scaler.fit_transform(tfidf_train_data)\n",
        "tfidf_scaled_test_data = max_abs_scaler.transform(tfidf_test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFwwrOQa2K86",
        "colab_type": "text"
      },
      "source": [
        "Fit the vectorized training data to a LinearSVC model, abd predict on the vectorized test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNHdgK-TUNcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_clf = LinearSVC(C=0.0001)\n",
        "tfidf_clf.fit(tfidf_scaled_train_data, train_labels)\n",
        "tfidf_predictions = tfidf_clf.predict(tfidf_scaled_test_data)\n",
        "# tfidf_proba = tfidf_clf.predict_proba(tfidf_scaled_test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWaQosh72ayj",
        "colab_type": "text"
      },
      "source": [
        "Compute accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu5XF4s8UNcN",
        "colab_type": "code",
        "outputId": "1ded3406-1c1e-47df-b4e2-ff1edde6d38e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(longer_test_labels, tfidf_predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.772"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}