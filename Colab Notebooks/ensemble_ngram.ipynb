{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "ensemble-ngram.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD7IFj9H8VuJ",
        "colab_type": "text"
      },
      "source": [
        "Mount Google Drive and import necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ3tQzgqXVJy",
        "colab_type": "code",
        "outputId": "b70d026a-2894-4887-9e82-b2150abc1f74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "data_dir = 'gdrive/My Drive/Colab Notebooks/AuthorshipAttribution/data' # @param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NQntCg2X4_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import codecs\n",
        "import operator\n",
        "import re\n",
        "import string\n",
        "import argparse\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pUHN800XVJ3",
        "colab_type": "code",
        "outputId": "5002c145-a46b-4f16-904f-00756e4634ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, WordPunctTokenizer,PunktSentenceTokenizer, TreebankWordTokenizer\n",
        "from nltk.corpus import stopwords, webtext\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('webtext')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrjJljnXXVJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale\n",
        "from sklearn import utils\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC ,SVC\n",
        "from sklearn import preprocessing\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTPt0B_L8U4W",
        "colab_type": "text"
      },
      "source": [
        "Function to calculate the n-gram frequency of a string of text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztF8npWsXVJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculating n-gram frequency for a review text\n",
        "def ngram_represent_text(text,n):\n",
        "    if n>0:\n",
        "        tokens = [text[i:i+n] for i in range(len(text)-n+1)]\n",
        "    frequency = defaultdict(int)\n",
        "    for token in tokens:\n",
        "        frequency[token] += 1\n",
        "    return frequency"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlNJxuCa8nPc",
        "colab_type": "text"
      },
      "source": [
        "Function to get the n-gram vocabulary of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p--iFy20XVKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# constructing n-gram vocabulary from texts - calculating n-gram frequencies and storing them in dictionaries\n",
        "def ngram_extract_vocabulary(texts,n,ft):\n",
        "    occurrences=defaultdict(int)\n",
        "    for text in texts:\n",
        "        text_occurrences=ngram_represent_text(text,n)\n",
        "        for ngram in text_occurrences:\n",
        "            if ngram in occurrences:\n",
        "                occurrences[ngram]+=text_occurrences[ngram]\n",
        "            else:\n",
        "                occurrences[ngram]=text_occurrences[ngram]\n",
        "    vocabulary=[]\n",
        "    for i in occurrences.keys():\n",
        "        if occurrences[i]>=ft:\n",
        "            vocabulary.append(i)\n",
        "    return vocabulary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2O2wB8j9cG3",
        "colab_type": "text"
      },
      "source": [
        "Set hyperparameters here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqOUDzdSXVKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "ft = 5          # low frequency threshold. only n-grams of frequency >= ft are added to the vocabulary\n",
        "n = 4           # n-gram value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAd_S7My8zN_",
        "colab_type": "text"
      },
      "source": [
        "We use the MaxAbsScaler to scale each feature by its Maximum Absolute Value. It translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfqx2VqkXVKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
        "stopwords_list = {'en': set(stopwords.words('english')) , 'fr':set(stopwords.words('french')),\n",
        "                  'sp': set(stopwords.words('spanish')) , 'it':set(stopwords.words('italian'))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8S1Fk9X80yY",
        "colab_type": "text"
      },
      "source": [
        "Load and preprocess training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNqPr91aXVKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# placeholders for training and testing data\n",
        "train_set , train_labels = [], []\n",
        "test_set , test_labels = [] , []\n",
        "\n",
        "# reading training and testing data from pickle files\n",
        "train_data, test_data = None, None\n",
        "with open(data_dir + \"/train_data.pickle\", \"rb\") as f:\n",
        "    train_data = pickle.load(f)\n",
        "with open(data_dir + \"/test_data.pickle\", \"rb\") as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "# populating training and testing placeholders with data\n",
        "train_set = train_data['train_texts']\n",
        "train_labels = train_data['train_labels']\n",
        "test_set = test_data['test_texts']\n",
        "test_labels = test_data['test_labels']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39zC_iq0XVKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_chunks(l, n):\n",
        "    n = max(1, n)\n",
        "    return [l[i:i+n] for i in range(0, len(l), n)]\n",
        "\n",
        "from statistics import mean\n",
        "word_counts = [text.count(\" \") for text in test_set]\n",
        "mean(word_counts)\n",
        "\n",
        "# 182 words is quite short\n",
        "# Try to join 5 tests texts together\n",
        "longer_test_texts = get_chunks(test_set, 5)\n",
        "longer_test_labels = get_chunks(test_labels, 5)\n",
        "\n",
        "all([len(set(x)) == 1 for x in longer_test_labels])  # Make sure that all combined labels are the same\n",
        "\n",
        "test_set = ['\\n'.join(chunk) for chunk in longer_test_texts]\n",
        "test_labels = [chunk[0] for chunk in longer_test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHJl_DDqXVKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "author_dict = {}\n",
        "n_parts = 10\n",
        "for review, author in zip(train_set, train_labels):\n",
        "    n_chars = len(review) // n_parts\n",
        "    author_dict[author] = [review[i:i+n_chars] for i in range(0, len(review), n_chars)]\n",
        "\n",
        "new_train_set, new_train_labels = [], []\n",
        "for author, reviews in author_dict.items():\n",
        "    new_train_set.extend(reviews)\n",
        "    new_train_labels.extend([author] * len(reviews))\n",
        "\n",
        "# for author, reviews in author_dict.items():\n",
        "#     print('author: {}\\t\\tnum_reviews: {}'.format(author, len(reviews)))\n",
        "# print('len(new_train_set): {}\\t\\tlen(new_train_labels): {}'.format(len(new_train_set), len(new_train_labels)))\n",
        "\n",
        "train_set = new_train_set\n",
        "train_labels = new_train_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad_-n22O8-yf",
        "colab_type": "text"
      },
      "source": [
        "Vectorize the training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYdNbOPYXVKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training and predicting using n-gram model (which uses SVC)\n",
        "ngram_vocabulary = ngram_extract_vocabulary(train_set , n , ft)\n",
        "ngram_vectorizer = CountVectorizer(strip_accents=False, analyzer='char',ngram_range=(n,n),lowercase=False,vocabulary=ngram_vocabulary)  \n",
        "ngram_train_data = ngram_vectorizer.fit_transform(train_set)\n",
        "ngram_train_data = ngram_train_data.astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6poKLb72XVKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(train_set)):\n",
        "    ngram_train_data[i]=ngram_train_data[i]/len(train_set[i])\n",
        "ngram_test_data = ngram_vectorizer.transform(test_set)\n",
        "ngram_test_data = ngram_test_data.astype(float)\n",
        "for i in range(len(test_set)):\n",
        "    ngram_test_data[i] = ngram_test_data[i]/len(test_set[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj70y3FJXVKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ngram_scaled_train_data = max_abs_scaler.fit_transform(ngram_train_data)\n",
        "ngram_scaled_test_data = max_abs_scaler.transform(ngram_test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-g4lMtj9CQW",
        "colab_type": "text"
      },
      "source": [
        "Fit the vectorized training data to an SVC model and predict on the vecorized testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMDkvlogXVKW",
        "colab_type": "code",
        "outputId": "888fdbb8-b096-4d71-ef72-697398452c84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ngram_clf = CalibratedClassifierCV(OneVsRestClassifier(SVC(C=0.01 , kernel='linear')))\n",
        "ngram_clf.fit(ngram_scaled_train_data, train_labels)\n",
        "ngram_predictions = ngram_clf.predict(ngram_scaled_test_data)\n",
        "ngram_proba = ngram_clf.predict_proba(ngram_scaled_test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eLHdtAz9JTA",
        "colab_type": "text"
      },
      "source": [
        "Compute accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qFkrf7EXVKZ",
        "colab_type": "code",
        "outputId": "3d2c6a30-6fb3-4048-c59b-db1861024810",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(test_labels, ngram_predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.996"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvZ9fYccXVKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}